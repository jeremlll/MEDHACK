{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for Beginner Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded! Shape: (12055680, 16)\n",
      "             timestamp                            patient_id first_name  \\\n",
      "0  2025-01-01 19:00:00  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "1  2025-01-01 19:00:05  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "2  2025-01-01 19:00:10  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "3  2025-01-01 19:00:15  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "4  2025-01-01 19:00:20  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "\n",
      "       last_name  age gender           address       city state  postcode  \\\n",
      "0  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "1  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "2  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "3  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "4  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "\n",
      "   diastolic_bp  systolic_bp  heart_rate  respiratory_rate  oxygen_saturation  \\\n",
      "0          79.9        118.3        74.4              17.1               98.6   \n",
      "1          79.2        119.0        75.8              16.0               96.0   \n",
      "2          80.8        119.6        74.3              16.5               96.8   \n",
      "3          81.3        120.2        75.1              16.9               96.9   \n",
      "4          79.9        118.2        75.9              16.3               97.2   \n",
      "\n",
      "   state_label  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "Test Data Loaded! Shape: (325440, 16)\n",
      "   ID            timestamp                            patient_id first_name  \\\n",
      "0   1  2025-01-01 19:00:00  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "1   2  2025-01-01 19:00:05  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "2   3  2025-01-01 19:00:10  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "3   4  2025-01-01 19:00:15  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "4   5  2025-01-01 19:00:20  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "\n",
      "   last_name  age gender            address      city state  postcode  \\\n",
      "0  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "1  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "2  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "3  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "4  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "\n",
      "   diastolic_bp  systolic_bp  heart_rate  respiratory_rate  oxygen_saturation  \n",
      "0          87.0        145.5       112.4              16.8               98.4  \n",
      "1          86.3        145.7       112.6              15.0               96.5  \n",
      "2          86.0        146.5       110.9              16.0               97.3  \n",
      "3          87.0        147.0       111.4              15.2               97.7  \n",
      "4          85.9        145.2       112.1              15.0               97.6  \n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Import Libraries & Load Data\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For our Deep Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "#For splitting the training and testing data\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "\n",
    "\n",
    "# Some display settings for nicer graphs\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "print(\"Data Loaded! Shape:\", train_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "print(\"Test Data Loaded! Shape:\", test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'patient_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 107\u001b[0m\n\u001b[1;32m    103\u001b[0m         y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(LabelEncoder()\u001b[39m.\u001b[39mfit_transform(df[\u001b[39m'\u001b[39m\u001b[39mstate_label\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m X, y, df\n\u001b[0;32m--> 107\u001b[0m X_train, y_train, train_df\u001b[39m=\u001b[39m convert_df(train_df)\n\u001b[1;32m    108\u001b[0m fc \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdiastolic_bp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msystolic_bp\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    109\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mheart_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrespiratory_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moxygen_saturation\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender_encoded\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    110\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mheart_rate_diff1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msystolic_bp_diff1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdiastolic_bp_diff1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m        \u001b[39m'\u001b[39m\u001b[39moxygen_delivery_diff3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpulse_pressure_diff3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    130\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mmean_arterial_pressure_diff3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrate_pressure_product_diff3\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mconvert_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Create lags\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m lag_features:\n\u001b[0;32m---> 21\u001b[0m     df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m_lag1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mpatient_id\u001b[39m\u001b[39m'\u001b[39m)[feature]\u001b[39m.\u001b[39mshift(\u001b[39m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m     df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m_lag2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mpatient_id\u001b[39m\u001b[39m'\u001b[39m)[feature]\u001b[39m.\u001b[39mshift(\u001b[39m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m     df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m_lag5\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mpatient_id\u001b[39m\u001b[39m'\u001b[39m)[feature]\u001b[39m.\u001b[39mshift(\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m by \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8870\u001b[0m     obj\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   8871\u001b[0m     keys\u001b[39m=\u001b[39mby,\n\u001b[1;32m   8872\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   8873\u001b[0m     level\u001b[39m=\u001b[39mlevel,\n\u001b[1;32m   8874\u001b[0m     as_index\u001b[39m=\u001b[39mas_index,\n\u001b[1;32m   8875\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m   8876\u001b[0m     group_keys\u001b[39m=\u001b[39mgroup_keys,\n\u001b[1;32m   8877\u001b[0m     observed\u001b[39m=\u001b[39mobserved,\n\u001b[1;32m   8878\u001b[0m     dropna\u001b[39m=\u001b[39mdropna,\n\u001b[1;32m   8879\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna \u001b[39m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[1;32m   1279\u001b[0m         obj,\n\u001b[1;32m   1280\u001b[0m         keys,\n\u001b[1;32m   1281\u001b[0m         axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   1282\u001b[0m         level\u001b[39m=\u001b[39mlevel,\n\u001b[1;32m   1283\u001b[0m         sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m   1284\u001b[0m         observed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m observed \u001b[39mis\u001b[39;00m lib\u001b[39m.\u001b[39mno_default \u001b[39melse\u001b[39;00m observed,\n\u001b[1;32m   1285\u001b[0m         dropna\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna,\n\u001b[1;32m   1286\u001b[0m     )\n\u001b[1;32m   1288\u001b[0m \u001b[39mif\u001b[39;00m observed \u001b[39mis\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(ping\u001b[39m.\u001b[39m_passed_categorical \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m grouper\u001b[39m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'patient_id'"
     ]
    }
   ],
   "source": [
    "feature_columns = ['age', 'diastolic_bp', 'systolic_bp', 'heart_rate',\n",
    "                  'respiratory_rate', 'oxygen_saturation']\n",
    "def convert_df(df):\n",
    "    columns_to_drop = ['timestamp', 'patient_id', 'first_name', 'last_name', \n",
    "                   'address', 'city', 'state', 'postcode']\n",
    "\n",
    "    # Drop the columns from the DataFrame\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    feature_columns = ['age', 'diastolic_bp', 'systolic_bp', 'heart_rate',\n",
    "                  'respiratory_rate', 'oxygen_saturation']\n",
    "    # Encode gender\n",
    "    df['gender_encoded'] = LabelEncoder().fit_transform(df['gender'])\n",
    "    feature_columns.append('gender_encoded')\n",
    "\n",
    "    # Assuming df is your dataframe and it's sorted by timestamp\n",
    "    lag_features = ['heart_rate', 'systolic_bp', 'diastolic_bp']\n",
    "\n",
    "    # Create lags\n",
    "    for feature in lag_features:\n",
    "        df[f'{feature}_lag1'] = df.groupby('patient_id')[feature].shift(1)\n",
    "        df[f'{feature}_lag2'] = df.groupby('patient_id')[feature].shift(2)\n",
    "        df[f'{feature}_lag5'] = df.groupby('patient_id')[feature].shift(5)\n",
    "        df[f'{feature}_lag10'] = df.groupby('patient_id')[feature].shift(10)\n",
    "\n",
    "    # Create differences between current and lagged values\n",
    "    for feature in lag_features:\n",
    "        df[f'{feature}_diff1'] = df[feature] - df[f'{feature}_lag1']\n",
    "        df[f'{feature}_diff2'] = df[feature] - df[f'{feature}_lag2']\n",
    "        df[f'{feature}_diff5'] = df[feature] - df[f'{feature}_lag5']\n",
    "        df[f'{feature}_diff10'] = df[feature] - df[f'{feature}_lag10']\n",
    "\n",
    "    # Add new features to feature columns\n",
    "    feature_columns.extend([f'{feature}_diff1' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_diff2' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_diff5' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_diff10' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag1' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag2' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag5' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag10' for feature in lag_features])\n",
    "\n",
    "\n",
    "    ## Feature lag and inference for derived features\n",
    "\n",
    "    # Original oxygen and blood pressure derived features\n",
    "    df['oxygen_delivery'] = df['heart_rate'] * df['oxygen_saturation'] * df['systolic_bp']\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = (2 * df['diastolic_bp'] + df['systolic_bp']) / 3\n",
    "    df['rate_pressure_product'] = df['heart_rate'] * df['systolic_bp']\n",
    "\n",
    "    # Define derived features for lagging\n",
    "    derived_features = ['oxygen_delivery', 'pulse_pressure', 'mean_arterial_pressure', 'rate_pressure_product']\n",
    "\n",
    "    # Create lags\n",
    "    for feature in derived_features:\n",
    "        df[f'{feature}_lag1'] = df.groupby('patient_id')[feature].shift(1)\n",
    "        df[f'{feature}_lag2'] = df.groupby('patient_id')[feature].shift(2)\n",
    "        df[f'{feature}_lag3'] = df.groupby('patient_id')[feature].shift(3)\n",
    "\n",
    "    # Create differences\n",
    "    for feature in derived_features:\n",
    "        # Difference with 1 timestep ago\n",
    "        df[f'{feature}_diff1'] = df[feature] - df[f'{feature}_lag1']\n",
    "        # Difference with 2 timesteps ago\n",
    "        df[f'{feature}_diff2'] = df[feature] - df[f'{feature}_lag2']\n",
    "        # Difference with 3 timesteps ago\n",
    "        df[f'{feature}_diff3'] = df[feature] - df[f'{feature}_lag3']\n",
    "\n",
    "    # Add all new features to feature columns\n",
    "    feature_columns.extend(derived_features)  # Add original derived features\n",
    "    feature_columns.extend([f'{feature}_lag1' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_lag2' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_lag3' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_diff1' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_diff2' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_diff3' for feature in derived_features])\n",
    "\n",
    "    df = df.fillna(method='bfill')\n",
    "\n",
    "\n",
    "\n",
    "    X = df[feature_columns]\n",
    "    if 'state_label' in df:\n",
    "        y = pd.Series(LabelEncoder().fit_transform(df['state_label']))\n",
    "    else:\n",
    "        y = None\n",
    "    return X, y, df\n",
    "\n",
    "\n",
    "def simple_preprocess(df, feature_columns, target_column='state_label', test_size=0.2, random_state=420):\n",
    "    # 1. Handle missing values using backward fill (if any NaN values remain)\n",
    "    df = df.fillna(method='bfill')\n",
    "\n",
    "    # 3. Scale the features while preserving outliers\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])  # Apply scaling\n",
    "\n",
    "    # 4. Prepare X and y\n",
    "    X = df[feature_columns, 'age', 'timestamp', 'patient_id']\n",
    "    y = None\n",
    "    if target_column in df:\n",
    "        y = pd.Series(LabelEncoder().fit_transform(df['state_label']))\n",
    "\n",
    "    return X, y, df\n",
    "\n",
    "X_train, y_train, train_df= convert_df(train_df)\n",
    "fc = ['diastolic_bp', 'systolic_bp',\n",
    "       'heart_rate', 'respiratory_rate', 'oxygen_saturation', 'gender_encoded',\n",
    "       'heart_rate_diff1', 'systolic_bp_diff1', 'diastolic_bp_diff1',\n",
    "       'heart_rate_diff2', 'systolic_bp_diff2', 'diastolic_bp_diff2',\n",
    "       'heart_rate_diff5', 'systolic_bp_diff5', 'diastolic_bp_diff5',\n",
    "       'heart_rate_diff10', 'systolic_bp_diff10', 'diastolic_bp_diff10',\n",
    "       'heart_rate_lag1', 'systolic_bp_lag1', 'diastolic_bp_lag1',\n",
    "       'heart_rate_lag2', 'systolic_bp_lag2', 'diastolic_bp_lag2',\n",
    "       'heart_rate_lag5', 'systolic_bp_lag5', 'diastolic_bp_lag5',\n",
    "       'heart_rate_lag10', 'systolic_bp_lag10', 'diastolic_bp_lag10',\n",
    "       'oxygen_delivery', 'pulse_pressure', 'mean_arterial_pressure',\n",
    "       'rate_pressure_product', 'oxygen_delivery_lag1', 'pulse_pressure_lag1',\n",
    "       'mean_arterial_pressure_lag1', 'rate_pressure_product_lag1',\n",
    "       'oxygen_delivery_lag2', 'pulse_pressure_lag2',\n",
    "       'mean_arterial_pressure_lag2', 'rate_pressure_product_lag2',\n",
    "       'oxygen_delivery_lag3', 'pulse_pressure_lag3',\n",
    "       'mean_arterial_pressure_lag3', 'rate_pressure_product_lag3',\n",
    "       'oxygen_delivery_diff1', 'pulse_pressure_diff1',\n",
    "       'mean_arterial_pressure_diff1', 'rate_pressure_product_diff1',\n",
    "       'oxygen_delivery_diff2', 'pulse_pressure_diff2',\n",
    "       'mean_arterial_pressure_diff2', 'rate_pressure_product_diff2',\n",
    "       'oxygen_delivery_diff3', 'pulse_pressure_diff3',\n",
    "       'mean_arterial_pressure_diff3', 'rate_pressure_product_diff3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train, train_df = simple_preprocess(train_df, fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the split (default is 75% train, 25% test) for holdout\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    test_size=0.25,     # Size of the test set (0.25 = 25% of data)\n",
    "    random_state=69,     # Set seed for reproducibility\n",
    "    shuffle=True         # Shuffle the data before splitting\n",
    ") # this is for model analysis later on if the val not being nice\n",
    "\n",
    "#2.4 Extract features (X) and labels (y) from the training set\n",
    "print(\"Training Features shape:\", X_train.shape)\n",
    "print(\"Training Labels shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = y_train.unique()\n",
    "num_classes = len(unique_states)\n",
    "print(\"Unique state labels:\", unique_states)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Sort them to create a mapping\n",
    "unique_states_sorted = sorted(unique_states)\n",
    "state_to_index = {state: i for i, state in enumerate(unique_states_sorted)}\n",
    "print(\"State to Index mapping:\", state_to_index)\n",
    "\n",
    "# Convert our training labels to numerical indices\n",
    "y_train_indices = y_train.map(state_to_index)\n",
    "\n",
    "# One-hot encode the training labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_encoded = to_categorical(y_train_indices, num_classes=num_classes)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape, \n",
    "      \"Example one-hot vector:\", y_train_encoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Clear the Keras session before model creation\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Input Layer\n",
    "        Dense(128, activation='selu', input_shape=input_shape,\n",
    "              kernel_regularizer=regularizers.l2(0.001),\n",
    "              kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Hidden Layer\n",
    "        Dense(64, activation='selu',\n",
    "              kernel_regularizer=regularizers.l2(0.001),\n",
    "              kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output Layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Simpler callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=(X_train.shape[1],), num_classes=num_classes)\n",
    "history = model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over Epochs\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, test_df = convert_df(test_df)\n",
    "#X_test, y_test, test_df = simple_preprocess(test_df, fc)\n",
    "\n",
    "#  Extract the ID column from the test DataFrame\n",
    "ids = test_df['ID']\n",
    "# Remove the ID column to get only the 5 vital sign features (diastolic_bp, systolic_bp, heart_rate, respiratory_rate, oxygen_saturation)\n",
    "X_test_features = test_df.drop('ID', axis=1)\n",
    "\n",
    "# Generate predictions (probabilities) for the test set using the correct feature set\n",
    "pred_probabilities = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted class indices\n",
    "pred_class_indices = np.argmax(pred_probabilities, axis=1)\n",
    "\n",
    "# Map back to original state labels (using the mapping from training)\n",
    "index_to_state = {v: k for k, v in state_to_index.items()}\n",
    "predicted_state_labels = [index_to_state[idx] for idx in pred_class_indices]\n",
    "\n",
    "# Create a DataFrame with the ID and predicted_label columns\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'predicted_label': predicted_state_labels\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictions_trial.csv', index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
