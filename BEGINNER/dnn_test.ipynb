{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for Beginner Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded! Shape: (12055680, 16)\n",
      "             timestamp                            patient_id first_name  \\\n",
      "0  2025-01-01 19:00:00  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "1  2025-01-01 19:00:05  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "2  2025-01-01 19:00:10  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "3  2025-01-01 19:00:15  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "4  2025-01-01 19:00:20  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "\n",
      "       last_name  age gender           address       city state  postcode  \\\n",
      "0  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "1  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "2  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "3  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "4  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "\n",
      "   diastolic_bp  systolic_bp  heart_rate  respiratory_rate  oxygen_saturation  \\\n",
      "0          79.9        118.3        74.4              17.1               98.6   \n",
      "1          79.2        119.0        75.8              16.0               96.0   \n",
      "2          80.8        119.6        74.3              16.5               96.8   \n",
      "3          81.3        120.2        75.1              16.9               96.9   \n",
      "4          79.9        118.2        75.9              16.3               97.2   \n",
      "\n",
      "   state_label  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "Test Data Loaded! Shape: (325440, 16)\n",
      "   ID            timestamp                            patient_id first_name  \\\n",
      "0   1  2025-01-01 19:00:00  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "1   2  2025-01-01 19:00:05  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "2   3  2025-01-01 19:00:10  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "3   4  2025-01-01 19:00:15  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "4   5  2025-01-01 19:00:20  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "\n",
      "   last_name  age gender            address      city state  postcode  \\\n",
      "0  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "1  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "2  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "3  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "4  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "\n",
      "   diastolic_bp  systolic_bp  heart_rate  respiratory_rate  oxygen_saturation  \n",
      "0          87.0        145.5       112.4              16.8               98.4  \n",
      "1          86.3        145.7       112.6              15.0               96.5  \n",
      "2          86.0        146.5       110.9              16.0               97.3  \n",
      "3          87.0        147.0       111.4              15.2               97.7  \n",
      "4          85.9        145.2       112.1              15.0               97.6  \n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Import Libraries & Load Data\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For our Deep Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "#For splitting the training and testing data\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Some display settings for nicer graphs\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "print(\"Data Loaded! Shape:\", train_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "print(\"Test Data Loaded! Shape:\", test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['age', 'diastolic_bp', 'systolic_bp', 'heart_rate',\n",
    "                  'respiratory_rate', 'oxygen_saturation']\n",
    "def convert_df(df):\n",
    "    feature_columns = ['age', 'diastolic_bp', 'systolic_bp', 'heart_rate',\n",
    "                  'respiratory_rate', 'oxygen_saturation']\n",
    "    # Encode gender\n",
    "    df['gender_encoded'] = LabelEncoder().fit_transform(df['gender'])\n",
    "    feature_columns.append('gender_encoded')\n",
    "\n",
    "    # Assuming df is your dataframe and it's sorted by timestamp\n",
    "    lag_features = ['heart_rate', 'systolic_bp', 'diastolic_bp']\n",
    "\n",
    "    # Create lags\n",
    "    for feature in lag_features:\n",
    "        df[f'{feature}_lag1'] = df.groupby('patient_id')[feature].shift(1)\n",
    "        df[f'{feature}_lag2'] = df.groupby('patient_id')[feature].shift(2)\n",
    "        df[f'{feature}_lag5'] = df.groupby('patient_id')[feature].shift(5)\n",
    "        df[f'{feature}_lag10'] = df.groupby('patient_id')[feature].shift(10)\n",
    "\n",
    "    # Create differences between current and lagged values\n",
    "    for feature in lag_features:\n",
    "        df[f'{feature}_diff1'] = df[feature] - df[f'{feature}_lag1']\n",
    "        df[f'{feature}_diff2'] = df[feature] - df[f'{feature}_lag2']\n",
    "        df[f'{feature}_diff5'] = df[feature] - df[f'{feature}_lag5']\n",
    "        df[f'{feature}_diff10'] = df[feature] - df[f'{feature}_lag10']\n",
    "\n",
    "    # Add new features to feature columns\n",
    "    feature_columns.extend([f'{feature}_diff1' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_diff2' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_diff5' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_diff10' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag1' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag2' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag5' for feature in lag_features])\n",
    "    feature_columns.extend([f'{feature}_lag10' for feature in lag_features])\n",
    "\n",
    "\n",
    "    ## Feature lag and inference for derived features\n",
    "\n",
    "    # Original oxygen and blood pressure derived features\n",
    "    df['oxygen_delivery'] = df['heart_rate'] * df['oxygen_saturation'] * df['systolic_bp']\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = (2 * df['diastolic_bp'] + df['systolic_bp']) / 3\n",
    "    df['rate_pressure_product'] = df['heart_rate'] * df['systolic_bp']\n",
    "\n",
    "    # Define derived features for lagging\n",
    "    derived_features = ['oxygen_delivery', 'pulse_pressure', 'mean_arterial_pressure', 'rate_pressure_product']\n",
    "\n",
    "    # Create lags\n",
    "    for feature in derived_features:\n",
    "        df[f'{feature}_lag1'] = df.groupby('patient_id')[feature].shift(1)\n",
    "        df[f'{feature}_lag2'] = df.groupby('patient_id')[feature].shift(2)\n",
    "        df[f'{feature}_lag3'] = df.groupby('patient_id')[feature].shift(3)\n",
    "\n",
    "    # Create differences\n",
    "    for feature in derived_features:\n",
    "        # Difference with 1 timestep ago\n",
    "        df[f'{feature}_diff1'] = df[feature] - df[f'{feature}_lag1']\n",
    "        # Difference with 2 timesteps ago\n",
    "        df[f'{feature}_diff2'] = df[feature] - df[f'{feature}_lag2']\n",
    "        # Difference with 3 timesteps ago\n",
    "        df[f'{feature}_diff3'] = df[feature] - df[f'{feature}_lag3']\n",
    "\n",
    "    # Add all new features to feature columns\n",
    "    feature_columns.extend(derived_features)  # Add original derived features\n",
    "    feature_columns.extend([f'{feature}_lag1' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_lag2' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_lag3' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_diff1' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_diff2' for feature in derived_features])\n",
    "    feature_columns.extend([f'{feature}_diff3' for feature in derived_features])\n",
    "\n",
    "    df = df.fillna(method='bfill')\n",
    "\n",
    "\n",
    "\n",
    "    X = df[feature_columns]\n",
    "    if 'state_label' in df:\n",
    "        y = pd.Series(LabelEncoder().fit_transform(df['state_label']))\n",
    "    else:\n",
    "        y = None\n",
    "    return X, y, df\n",
    "\n",
    "\n",
    "def simple_preprocess(df, target_column='state_label', test_size=0.2, random_state=42):\n",
    "    # 1. Handle missing values using backward fill (as you've done in convert_df)\n",
    "    df = df.fillna(method='bfill')  # Backward fill if any remaining NaN values\n",
    "    \n",
    "    # 2. Scale the features (excluding the target column) while preserving outliers\n",
    "    scaler = StandardScaler()\n",
    "    features = [col for col in df.columns if col != target_column]  # Exclude the target column\n",
    "    df[features] = scaler.fit_transform(df[features])  # Apply scaling\n",
    "    \n",
    "    # 3. Split the data into features and target\n",
    "    X = df[features]  # Features\n",
    "    y = df[target_column]  # Target\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, scaler, features\n",
    "\n",
    "X_train, y_train, train_df = convert_df(train_df)\n",
    "X_train, X_val, y_train, y_val, scaler, features = simple_preprocess(train_df, target_column='state_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the split (default is 75% train, 25% test) for holdout\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    test_size=0.25,     # Size of the test set (0.25 = 25% of data)\n",
    "    random_state=69,     # Set seed for reproducibility\n",
    "    shuffle=True         # Shuffle the data before splitting\n",
    ") # this is for model analysis later on if the val not being nice\n",
    "\n",
    "#2.4 Extract features (X) and labels (y) from the training set\n",
    "print(\"Training Features shape:\", X_train.shape)\n",
    "print(\"Training Labels shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = y_train.unique()\n",
    "num_classes = len(unique_states)\n",
    "print(\"Unique state labels:\", unique_states)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Sort them to create a mapping\n",
    "unique_states_sorted = sorted(unique_states)\n",
    "state_to_index = {state: i for i, state in enumerate(unique_states_sorted)}\n",
    "print(\"State to Index mapping:\", state_to_index)\n",
    "\n",
    "# Convert our training labels to numerical indices\n",
    "y_train_indices = y_train.map(state_to_index)\n",
    "\n",
    "# One-hot encode the training labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_encoded = to_categorical(y_train_indices, num_classes=num_classes)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape, \n",
    "      \"Example one-hot vector:\", y_train_encoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Clear the Keras session before model creation\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer with L2 Regularization and Batch Normalization\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],),\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Hidden Layer 3\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer for Multi-class Classification\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,  # Stop after 5 epochs of no improvement\n",
    "    restore_best_weights=True,  # Roll back to the best weights\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce the learning rate by half\n",
    "    patience=3,  # Wait for 3 epochs of no improvement\n",
    "    min_lr=1e-5,  # Set a lower bound on the learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Summary of the Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,  # You can adjust this\n",
    "    batch_size=128,  # You can experiment with different batch sizes\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over Epochs\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, test_df = convert_df(test_df)\n",
    "#  Extract the ID column from the test DataFrame\n",
    "ids = test_df['ID']\n",
    "# Remove the ID column to get only the 5 vital sign features (diastolic_bp, systolic_bp, heart_rate, respiratory_rate, oxygen_saturation)\n",
    "X_test_features = test_df.drop('ID', axis=1)\n",
    "\n",
    "# Generate predictions (probabilities) for the test set using the correct feature set\n",
    "pred_probabilities = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted class indices\n",
    "pred_class_indices = np.argmax(pred_probabilities, axis=1)\n",
    "\n",
    "# Map back to original state labels (using the mapping from training)\n",
    "index_to_state = {v: k for k, v in state_to_index.items()}\n",
    "predicted_state_labels = [index_to_state[idx] for idx in pred_class_indices]\n",
    "\n",
    "# Create a DataFrame with the ID and predicted_label columns\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'predicted_label': predicted_state_labels\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictions_trial.csv', index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
