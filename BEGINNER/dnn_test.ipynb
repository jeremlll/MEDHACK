{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for Beginner Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded! Shape: (12055680, 16)\n",
      "             timestamp                            patient_id first_name  \\\n",
      "0  2025-01-01 19:00:00  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "1  2025-01-01 19:00:05  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "2  2025-01-01 19:00:10  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "3  2025-01-01 19:00:15  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "4  2025-01-01 19:00:20  b317e7ee-8af7-3e9c-3e0f-646395b8c81a  Howard613   \n",
      "\n",
      "       last_name  age gender           address       city state  postcode  \\\n",
      "0  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "1  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "2  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "3  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "4  Altenwerth646   42      M  2/58 JASPER ROAD  BENTLEIGH   VIC      3204   \n",
      "\n",
      "   diastolic_bp  systolic_bp  heart_rate  respiratory_rate  oxygen_saturation  \\\n",
      "0          79.9        118.3        74.4              17.1               98.6   \n",
      "1          79.2        119.0        75.8              16.0               96.0   \n",
      "2          80.8        119.6        74.3              16.5               96.8   \n",
      "3          81.3        120.2        75.1              16.9               96.9   \n",
      "4          79.9        118.2        75.9              16.3               97.2   \n",
      "\n",
      "   state_label  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "Test Data Loaded! Shape: (325440, 16)\n",
      "   ID            timestamp                            patient_id first_name  \\\n",
      "0   1  2025-01-01 19:00:00  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "1   2  2025-01-01 19:00:05  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "2   3  2025-01-01 19:00:10  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "3   4  2025-01-01 19:00:15  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "4   5  2025-01-01 19:00:20  d8cdccaa-9bc6-4b08-ee28-7dfaa0f07caf  Sammie902   \n",
      "\n",
      "   last_name  age gender            address      city state  postcode  \\\n",
      "0  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "1  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "2  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "3  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "4  Brakus656   11      M  31 CHELSEA STREET  BRIGHTON   VIC      3186   \n",
      "\n",
      "   diastolic_bp  systolic_bp  heart_rate  respiratory_rate  oxygen_saturation  \n",
      "0          87.0        145.5       112.4              16.8               98.4  \n",
      "1          86.3        145.7       112.6              15.0               96.5  \n",
      "2          86.0        146.5       110.9              16.0               97.3  \n",
      "3          87.0        147.0       111.4              15.2               97.7  \n",
      "4          85.9        145.2       112.1              15.0               97.6  \n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Import Libraries & Load Data\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For our Deep Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "#For splitting the training and testing data\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Some display settings for nicer graphs\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load dataset\n",
    "dataset_df = pd.read_csv('train_data.csv')\n",
    "print(\"Data Loaded! Shape:\", dataset_df.shape)\n",
    "print(dataset_df.head())\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "print(\"Test Data Loaded! Shape:\", test_df.shape)\n",
    "print(test_df.head())\n",
    "\n",
    "\n",
    "# 2.1: Drop columns we don't want or need from training data\n",
    "# They might not directly help us predict in a simple DNN approach.\n",
    "dataset_df = dataset_df.drop([\n",
    "    'timestamp', 'patient_id', 'first_name', 'last_name', \n",
    "    'age', 'address', 'gender', 'city', 'state', 'postcode'\n",
    "], axis=1)\n",
    "\n",
    "test_df = test_df.drop([\n",
    "    'timestamp', 'patient_id', 'first_name', 'last_name', \n",
    "    'age', 'address', 'gender', 'city', 'state', 'postcode'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      " diastolic_bp         0\n",
      "systolic_bp          0\n",
      "heart_rate           0\n",
      "respiratory_rate     0\n",
      "oxygen_saturation    0\n",
      "state_label          0\n",
      "dtype: int64\n",
      "Training Features shape: (9041760, 5)\n",
      "Training Labels shape: (9041760,)\n",
      "Training Features shape: (3013920, 5)\n",
      "Training Labels shape: (3013920,)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 2. Preprocessing\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# 2.1: Check for missing values\n",
    "print(\"Missing values in training data:\\n\", dataset_df.isna().sum())\n",
    "# print(\"Missing values in test data:\\n\", test_df.isna().sum())\n",
    "\n",
    "# Example strategy: just drop rows with missing data.\n",
    "# (Real-world might do more nuanced imputation.)\n",
    "dataset_df = dataset_df.dropna()\n",
    "# test_df = test_df.dropna()\n",
    "\n",
    "# 2.2: Extract features (X) and labels (y) from the training set\n",
    "X_all = dataset_df.drop('state_label', axis=1)\n",
    "y_all = dataset_df['state_label']\n",
    "\n",
    "# Perform the split (default is 75% train, 25% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, \n",
    "    y_all,\n",
    "    test_size=0.25,     # Size of the test set (0.25 = 25% of data)\n",
    "    random_state=42,     # Set seed for reproducibility\n",
    "    shuffle=True         # Shuffle the data before splitting\n",
    ")\n",
    "\n",
    "#2.4 Extract features (X) and labels (y) from the training set\n",
    "print(\"Training Features shape:\", X_train.shape)\n",
    "print(\"Training Labels shape:\", y_train.shape)\n",
    "\n",
    "\n",
    "#2.4 Extract features (X) and labels (y) from the testing set\n",
    "print(\"Training Features shape:\", X_test.shape)\n",
    "print(\"Training Labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique state labels: [0 1 2 3]\n",
      "Number of classes: 4\n",
      "State to Index mapping: {0: 0, 1: 1, 2: 2, 3: 3}\n",
      "y_train_encoded shape: (9041760, 4) Example one-hot vector: [1. 0. 0. 0.]\n",
      "Unique state labels: [0 2 1 3]\n",
      "Number of classes: 4\n",
      "State to Index mapping: {0: 0, 1: 1, 2: 2, 3: 3}\n",
      "y_train_encoded shape: (3013920, 4) Example one-hot vector: [1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "unique_states = y_train.unique()\n",
    "num_classes = len(unique_states)\n",
    "print(\"Unique state labels:\", unique_states)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Sort them to create a mapping\n",
    "unique_states_sorted = sorted(unique_states)\n",
    "state_to_index = {state: i for i, state in enumerate(unique_states_sorted)}\n",
    "print(\"State to Index mapping:\", state_to_index)\n",
    "\n",
    "# Convert our training labels to numerical indices\n",
    "y_train_indices = y_train.map(state_to_index)\n",
    "\n",
    "# One-hot encode the training labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_encoded = to_categorical(y_train_indices, num_classes=num_classes)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape, \n",
    "      \"Example one-hot vector:\", y_train_encoded[0])\n",
    "\n",
    "\n",
    "#Repeat for testing data\n",
    "unique_states = y_test.unique()\n",
    "num_classes = len(unique_states)\n",
    "print(\"Unique state labels:\", unique_states)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Sort them to create a mapping\n",
    "unique_states_sorted = sorted(unique_states)\n",
    "state_to_index = {state: i for i, state in enumerate(unique_states_sorted)}\n",
    "print(\"State to Index mapping:\", state_to_index)\n",
    "\n",
    "# Convert our testing labels to numerical indices\n",
    "y_test_indices = y_test.map(state_to_index)\n",
    "\n",
    "y_test_encoded = to_categorical(y_test_indices, num_classes=num_classes)\n",
    "print(\"y_train_encoded shape:\", y_test_encoded.shape, \n",
    "      \"Example one-hot vector:\", y_test_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer with tunable units\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=hp.Int('units_input', min_value=32, max_value=128, step=32),\n",
    "            activation='relu',\n",
    "            input_shape=(X_train.shape[1],)\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(hp.Float('dropout_input', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Hidden layer 1 with tunable regularization\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=hp.Int('units_hidden1', min_value=32, max_value=128, step=32),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(hp.Float('l2_hidden1', min_value=0.001, max_value=0.01, step=0.001))\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(hp.Float('dropout_hidden1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Hidden layer 2\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=hp.Int('units_hidden2', min_value=32, max_value=128, step=32),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(hp.Float('l2_hidden2', min_value=0.001, max_value=0.01, step=0.001))\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(hp.Float('dropout_hidden2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Output layer for multi-class classification\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a tunable optimizer\n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', ['adam', 'sgd']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='dnn_tuning'\n",
    ")\n",
    "\n",
    "# **Tune for batch size as well**\n",
    "tuner.search(\n",
    "    X_train, \n",
    "    y_train_encoded, \n",
    "    validation_split=0.2, \n",
    "    epochs=10, \n",
    "    verbose=1,\n",
    "    # Add a batch size search\n",
    "    batch_size=kt.HyperParameters().Choice('batch_size', [32, 64, 128, 256, 512])\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\", best_hps.values)\n",
    "\n",
    "# Retrieve the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# **Use the best batch size found by Keras Tuner**\n",
    "best_batch_size = best_hps.get('batch_size')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train_encoded,\n",
    "    epochs=500,\n",
    "    batch_size=best_batch_size,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Plotting training and validation performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over Epochs\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10170/10170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 296us/step\n",
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "#  Extract the ID column from the test DataFrame\n",
    "ids = test_df['ID']\n",
    "# Remove the ID column to get only the 5 vital sign features (diastolic_bp, systolic_bp, heart_rate, respiratory_rate, oxygen_saturation)\n",
    "X_test_features = test_df.drop('ID', axis=1)\n",
    "\n",
    "# Generate predictions (probabilities) for the test set using the correct feature set\n",
    "pred_probabilities = model.predict(X_test_features)\n",
    "\n",
    "# Convert probabilities to predicted class indices\n",
    "pred_class_indices = np.argmax(pred_probabilities, axis=1)\n",
    "\n",
    "# Map back to original state labels (using the mapping from training)\n",
    "index_to_state = {v: k for k, v in state_to_index.items()}\n",
    "predicted_state_labels = [index_to_state[idx] for idx in pred_class_indices]\n",
    "\n",
    "# Create a DataFrame with the ID and predicted_label columns\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'predicted_label': predicted_state_labels\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictions_tuned.csv', index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
